<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://savicktso.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://savicktso.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-14T10:37:44+00:00</updated><id>https://savicktso.github.io/feed.xml</id><title type="html">blank</title><subtitle>You got me! </subtitle><entry><title type="html">Share my current Raytune+mlflow workflow for model training</title><link href="https://savicktso.github.io/blog/2024/post-workflow/" rel="alternate" type="text/html" title="Share my current Raytune+mlflow workflow for model training"/><published>2024-04-11T18:26:00+00:00</published><updated>2024-04-11T18:26:00+00:00</updated><id>https://savicktso.github.io/blog/2024/post-workflow</id><content type="html" xml:base="https://savicktso.github.io/blog/2024/post-workflow/"><![CDATA[<p>When training a model, we would like to make sure we reached the its best performance with the most suitable combination of hyperparameters. Grid search can be enough for architectures with less variables to tune, but when you architecture is getting complex, this workflow can be definitely helpful to monitor everything.</p> <p>Raytune is a part of Ray library,it has several common used hyperparameter searching algorithms to help your model to get the optimal performance. In the meantime, mlflow is a visualization tool to make graphs for every parameter you want to watch. It is good to have a GUI like mlflow to track every run when you are in a huge project with many experiments to check.</p> <p>I would like to share my experience to select params for monitoring. Raytune usually has your validation accuracy/loss reported to represent the results. I also recommend you to add all the hyperparams you want to tune to trace the trend. Additionally, it is a good idea to add training loss to make sure you have the results of the training process.</p> <p><strong>But</strong>, one thing important to notice is that: The validation accuracy is not the actual generalizable accuracy!</p> <p>You should always have a separated test set to run the final test and make sure the model can be generalized instead of tuning for only the validation set. This point is pretty tricky and also got me to believe that the validation accuracy is the final accuracy. I guess this kind of division could be even more essential when we do pre-train on a larger dataset.</p>]]></content><author><name></name></author><category term="workflow"/><category term="workflow"/><summary type="html"><![CDATA[First blog post!]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://savicktso.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://savicktso.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://savicktso.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>